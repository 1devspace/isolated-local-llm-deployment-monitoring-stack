"use strict";(self.webpackChunkdocs_temp=self.webpackChunkdocs_temp||[]).push([[976],{2053:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>s,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"intro","title":"Getting Started","description":"Welcome to the Isolated Local LLM Deployment & Monitoring Stack! This guide will help you deploy a complete Large Language Model (LLM) environment on your local machine with Docker, featuring strict network isolation and comprehensive monitoring.","source":"@site/docs/intro.md","sourceDirName":".","slug":"/intro","permalink":"/Isolated-Local-LLM-Deployment-Monitoring-Stack/docs/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/mohamedaminehamdi/Isolated-Local-LLM-Deployment-Monitoring-Stack/tree/main/docs/docs/intro.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","next":{"title":"LLM Deployment Guide","permalink":"/Isolated-Local-LLM-Deployment-Monitoring-Stack/docs/llm-deployment"}}');var i=t(4848),r=t(8453);const s={sidebar_position:1},l="Getting Started",a={},d=[{value:"\ud83c\udfaf What You&#39;ll Get",id:"-what-youll-get",level:2},{value:"\ud83d\udccb Prerequisites",id:"-prerequisites",level:2},{value:"Required Software",id:"required-software",level:3},{value:"System Requirements",id:"system-requirements",level:3},{value:"\u26a1 Quick Start",id:"-quick-start",level:2},{value:"1. Clone the Repository",id:"1-clone-the-repository",level:3},{value:"2. Deploy the LLM Stack",id:"2-deploy-the-llm-stack",level:3},{value:"3. Access Your LLM",id:"3-access-your-llm",level:3},{value:"\ud83d\udd0d What&#39;s Next?",id:"-whats-next",level:2},{value:"\ud83d\udedf Need Help?",id:"-need-help",level:2}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"getting-started",children:"Getting Started"})}),"\n",(0,i.jsxs)(n.p,{children:["Welcome to the ",(0,i.jsx)(n.strong,{children:"Isolated Local LLM Deployment & Monitoring Stack"}),"! This guide will help you deploy a complete Large Language Model (LLM) environment on your local machine with Docker, featuring strict network isolation and comprehensive monitoring."]}),"\n",(0,i.jsx)(n.h2,{id:"-what-youll-get",children:"\ud83c\udfaf What You'll Get"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Ollama"}),": Local LLM runtime (supports LLaMA 3, Code Llama, and more)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Open WebUI"}),": Beautiful chat interface for interacting with your models"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Prometheus"}),": Time-series metrics collection and alerting"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Grafana"}),": Interactive dashboards and visualizations"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"cAdvisor"}),": Container-level resource monitoring"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Node Exporter"}),": Host-level system metrics"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"-prerequisites",children:"\ud83d\udccb Prerequisites"}),"\n",(0,i.jsx)(n.p,{children:"Before you begin, ensure you have:"}),"\n",(0,i.jsx)(n.h3,{id:"required-software",children:"Required Software"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Docker Desktop"}),": The foundation of our containerized deployment","\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"brew install --cask docker\nopen /Applications/Docker.app\n"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"system-requirements",children:"System Requirements"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"macOS"}),": 10.15 or later (Intel or Apple Silicon)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Memory"}),": At least 8GB RAM (16GB recommended for larger models)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Storage"}),": 20GB free space (models can be large)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Network"}),": No internet required after initial setup (air-gapped compatible)"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"-quick-start",children:"\u26a1 Quick Start"}),"\n",(0,i.jsx)(n.p,{children:"Get your LLM environment running in under 5 minutes:"}),"\n",(0,i.jsx)(n.h3,{id:"1-clone-the-repository",children:"1. Clone the Repository"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/mohamedaminehamdi/Isolated-Local-LLM-Deployment-Monitoring-Stack.git\ncd Isolated-Local-LLM-Deployment-Monitoring-Stack\n"})}),"\n",(0,i.jsx)(n.h3,{id:"2-deploy-the-llm-stack",children:"2. Deploy the LLM Stack"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Create isolated network\ndocker network create ollama-net\n\n# Deploy Ollama (LLM backend)\ndocker run -d \\\n  --name ollama \\\n  --network ollama-net \\\n  -p 127.0.0.1:11444:11434 \\\n  -v ollama-data:/root/.ollama \\\n  ollama/ollama\n\n# Load your first model (LLaMA 3)\ndocker exec -it ollama ollama pull llama3\n\n# Deploy Open WebUI (chat interface)\ndocker run -d \\\n  --name open-webui \\\n  --network ollama-net \\\n  -p 8081:8080 \\\n  -e OLLAMA_BASE_URL=http://ollama:11434 \\\n  -v open-webui:/app/backend/data \\\n  --restart always \\\n  ghcr.io/open-webui/open-webui:main\n"})}),"\n",(0,i.jsx)(n.h3,{id:"3-access-your-llm",children:"3. Access Your LLM"}),"\n",(0,i.jsxs)(n.p,{children:["\ud83c\udf89 ",(0,i.jsx)(n.strong,{children:"You're ready!"})," Open your browser and go to:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Chat Interface"}),": ",(0,i.jsx)(n.a,{href:"http://localhost:8081",children:"http://localhost:8081"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Create your first account"})," and start chatting with LLaMA 3"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"-whats-next",children:"\ud83d\udd0d What's Next?"}),"\n",(0,i.jsx)(n.p,{children:"Now that you have the basic LLM stack running, explore these advanced features:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"./llm-deployment",children:(0,i.jsx)(n.strong,{children:"LLM Deployment Guide"})}),": Deploy additional models and configure advanced settings"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"./monitoring",children:(0,i.jsx)(n.strong,{children:"Monitoring Stack"})}),": Add Prometheus, Grafana, and system monitoring"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"./security",children:(0,i.jsx)(n.strong,{children:"Security & Isolation"})}),": Understand the network isolation and security features"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"./troubleshooting",children:(0,i.jsx)(n.strong,{children:"Troubleshooting"})}),": Common issues and solutions"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"-need-help",children:"\ud83d\udedf Need Help?"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\ud83d\udcd6 Check our ",(0,i.jsx)(n.a,{href:"./troubleshooting",children:"Troubleshooting Guide"})]}),"\n",(0,i.jsxs)(n.li,{children:["\ud83d\udc1b ",(0,i.jsx)(n.a,{href:"https://github.com/mohamedaminehamdi/Isolated-Local-LLM-Deployment-Monitoring-Stack/issues",children:"Report Issues"})]}),"\n",(0,i.jsxs)(n.li,{children:["\ud83d\udcac ",(0,i.jsx)(n.a,{href:"https://github.com/mohamedaminehamdi/Isolated-Local-LLM-Deployment-Monitoring-Stack/discussions",children:"Discussions"})]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.admonition,{title:"Success Tip",type:"tip",children:(0,i.jsx)(n.p,{children:"Keep your models and data persistent by using Docker volumes. This way, your conversations and downloaded models survive container restarts!"})})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>l});var o=t(6540);const i={},r=o.createContext(i);function s(e){const n=o.useContext(r);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),o.createElement(r.Provider,{value:n},e.children)}}}]);