"use strict";(self.webpackChunkdocs_temp=self.webpackChunkdocs_temp||[]).push([[538],{7480:(e,n,l)=>{l.r(n),l.d(n,{assets:()=>d,contentTitle:()=>i,default:()=>h,frontMatter:()=>r,metadata:()=>a,toc:()=>t});const a=JSON.parse('{"id":"llm-deployment","title":"LLM Deployment Guide","description":"This guide covers deploying and configuring the LLM components: Ollama (the LLM runtime) and Open WebUI (the chat interface).","source":"@site/docs/llm-deployment.md","sourceDirName":".","slug":"/llm-deployment","permalink":"/Isolated-Local-LLM-Deployment-Monitoring-Stack/docs/llm-deployment","draft":false,"unlisted":false,"editUrl":"https://github.com/mohamedaminehamdi/Isolated-Local-LLM-Deployment-Monitoring-Stack/tree/main/docs/docs/llm-deployment.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Getting Started","permalink":"/Isolated-Local-LLM-Deployment-Monitoring-Stack/docs/intro"},"next":{"title":"Monitoring Stack Setup","permalink":"/Isolated-Local-LLM-Deployment-Monitoring-Stack/docs/monitoring"}}');var s=l(4848),o=l(8453);const r={sidebar_position:2},i="LLM Deployment Guide",d={},t=[{value:"\ud83d\udd39 Step 1: Isolated Docker Network",id:"-step-1-isolated-docker-network",level:2},{value:"\ud83d\udd39 Step 2: Deploy Ollama (LLM Backend)",id:"-step-2-deploy-ollama-llm-backend",level:2},{value:"Configuration Details:",id:"configuration-details",level:3},{value:"Verify Ollama is Running:",id:"verify-ollama-is-running",level:3},{value:"\ud83d\udd39 Step 3: Download Language Models",id:"-step-3-download-language-models",level:2},{value:"Download LLaMA 3 (Recommended)",id:"download-llama-3-recommended",level:3},{value:"Available Models",id:"available-models",level:3},{value:"Custom Models",id:"custom-models",level:3},{value:"\ud83d\udd39 Step 4: Deploy Open WebUI",id:"-step-4-deploy-open-webui",level:2},{value:"Configuration Details:",id:"configuration-details-1",level:3},{value:"\ud83c\udfaf Access Your LLM",id:"-access-your-llm",level:2},{value:"\u2699\ufe0f Advanced Configuration",id:"\ufe0f-advanced-configuration",level:2},{value:"Environment Variables",id:"environment-variables",level:3},{value:"GPU Acceleration (NVIDIA)",id:"gpu-acceleration-nvidia",level:3},{value:"Resource Limits",id:"resource-limits",level:3},{value:"\ud83d\udd27 Management Commands",id:"-management-commands",level:2},{value:"Container Management",id:"container-management",level:3},{value:"Model Management",id:"model-management",level:3},{value:"Data Backup",id:"data-backup",level:3},{value:"\ud83d\udea8 Troubleshooting",id:"-troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Performance Issues",id:"performance-issues",level:3}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"llm-deployment-guide",children:"LLM Deployment Guide"})}),"\n",(0,s.jsxs)(n.p,{children:["This guide covers deploying and configuring the LLM components: ",(0,s.jsx)(n.strong,{children:"Ollama"})," (the LLM runtime) and ",(0,s.jsx)(n.strong,{children:"Open WebUI"})," (the chat interface)."]}),"\n",(0,s.jsx)(n.h2,{id:"-step-1-isolated-docker-network",children:"\ud83d\udd39 Step 1: Isolated Docker Network"}),"\n",(0,s.jsx)(n.p,{children:"Create a dedicated network for the LLM components to ensure isolation:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"docker network create ollama-net\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Why isolation matters:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Keeps LLM containers separate from other services"}),"\n",(0,s.jsx)(n.li,{children:"Enables secure internal communication"}),"\n",(0,s.jsx)(n.li,{children:"Prevents unauthorized access to LLM services"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"-step-2-deploy-ollama-llm-backend",children:"\ud83d\udd39 Step 2: Deploy Ollama (LLM Backend)"}),"\n",(0,s.jsx)(n.p,{children:"Ollama serves as the backend that runs the actual language models:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"docker run -d \\\n  --name ollama \\\n  --network ollama-net \\\n  -p 127.0.0.1:11444:11434 \\\n  -v ollama-data:/root/.ollama \\\n  ollama/ollama\n"})}),"\n",(0,s.jsx)(n.h3,{id:"configuration-details",children:"Configuration Details:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Parameter"}),(0,s.jsx)(n.th,{children:"Purpose"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--network ollama-net"})}),(0,s.jsx)(n.td,{children:"Isolated network for LLM services"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"-p 127.0.0.1:11444:11434"})}),(0,s.jsx)(n.td,{children:"Expose API only to localhost (security)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"-v ollama-data:/root/.ollama"})}),(0,s.jsx)(n.td,{children:"Persistent storage for models"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"verify-ollama-is-running",children:"Verify Ollama is Running:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"curl http://localhost:11444/api/version\n"})}),"\n",(0,s.jsx)(n.p,{children:"Expected response:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{"version":"0.1.x"}\n'})}),"\n",(0,s.jsx)(n.h2,{id:"-step-3-download-language-models",children:"\ud83d\udd39 Step 3: Download Language Models"}),"\n",(0,s.jsx)(n.h3,{id:"download-llama-3-recommended",children:"Download LLaMA 3 (Recommended)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"docker exec -it ollama ollama pull llama3\n"})}),"\n",(0,s.jsx)(n.h3,{id:"available-models",children:"Available Models"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Model"}),(0,s.jsx)(n.th,{children:"Size"}),(0,s.jsx)(n.th,{children:"Best For"}),(0,s.jsx)(n.th,{children:"Command"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"LLaMA 3"})}),(0,s.jsx)(n.td,{children:"~4.7GB"}),(0,s.jsx)(n.td,{children:"General conversations"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"ollama pull llama3"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Code Llama"})}),(0,s.jsx)(n.td,{children:"~3.8GB"}),(0,s.jsx)(n.td,{children:"Code generation"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"ollama pull codellama"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Mistral"})}),(0,s.jsx)(n.td,{children:"~4.1GB"}),(0,s.jsx)(n.td,{children:"Fast responses"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"ollama pull mistral"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Vicuna"})}),(0,s.jsx)(n.td,{children:"~3.9GB"}),(0,s.jsx)(n.td,{children:"Instruction following"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"ollama pull vicuna"})})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"custom-models",children:"Custom Models"}),"\n",(0,s.jsx)(n.p,{children:"You can also use custom models or specific versions:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Specific model versions\ndocker exec -it ollama ollama pull llama3:8b\ndocker exec -it ollama ollama pull llama3:70b  # Requires ~40GB RAM\n\n# List downloaded models\ndocker exec -it ollama ollama list\n"})}),"\n",(0,s.jsx)(n.h2,{id:"-step-4-deploy-open-webui",children:"\ud83d\udd39 Step 4: Deploy Open WebUI"}),"\n",(0,s.jsx)(n.p,{children:"The web interface that provides a ChatGPT-like experience:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"docker run -d \\\n  --name open-webui \\\n  --network ollama-net \\\n  -p 8081:8080 \\\n  -e OLLAMA_BASE_URL=http://ollama:11434 \\\n  -v open-webui:/app/backend/data \\\n  --restart always \\\n  ghcr.io/open-webui/open-webui:main\n"})}),"\n",(0,s.jsx)(n.h3,{id:"configuration-details-1",children:"Configuration Details:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Parameter"}),(0,s.jsx)(n.th,{children:"Purpose"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--network ollama-net"})}),(0,s.jsx)(n.td,{children:"Same network as Ollama for communication"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"-p 8081:8080"})}),(0,s.jsx)(n.td,{children:"Web interface accessible at localhost:8081"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"-e OLLAMA_BASE_URL=..."})}),(0,s.jsx)(n.td,{children:"Tells WebUI how to reach Ollama"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"-v open-webui:/app/backend/data"})}),(0,s.jsx)(n.td,{children:"Persistent chat history and settings"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--restart always"})}),(0,s.jsx)(n.td,{children:"Auto-restart if container stops"})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"-access-your-llm",children:"\ud83c\udfaf Access Your LLM"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Open your browser"}),": Navigate to ",(0,s.jsx)(n.a,{href:"http://localhost:8081",children:"http://localhost:8081"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Create account"}),": Set up your first user account"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Select model"}),": Choose from your downloaded models"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Start chatting"}),": Begin your conversation with the LLM!"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"\ufe0f-advanced-configuration",children:"\u2699\ufe0f Advanced Configuration"}),"\n",(0,s.jsx)(n.h3,{id:"environment-variables",children:"Environment Variables"}),"\n",(0,s.jsx)(n.p,{children:"Customize Open WebUI behavior:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'docker run -d \\\n  --name open-webui \\\n  --network ollama-net \\\n  -p 8081:8080 \\\n  -e OLLAMA_BASE_URL=http://ollama:11434 \\\n  -e WEBUI_NAME="My Local LLM" \\\n  -e DEFAULT_MODELS="llama3,codellama" \\\n  -v open-webui:/app/backend/data \\\n  --restart always \\\n  ghcr.io/open-webui/open-webui:main\n'})}),"\n",(0,s.jsx)(n.h3,{id:"gpu-acceleration-nvidia",children:"GPU Acceleration (NVIDIA)"}),"\n",(0,s.jsx)(n.p,{children:"If you have an NVIDIA GPU, enable GPU acceleration for faster inference:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Ensure NVIDIA Container Toolkit is installed\n# Then run Ollama with GPU support:\n\ndocker run -d \\\n  --name ollama \\\n  --network ollama-net \\\n  --gpus all \\\n  -p 127.0.0.1:11444:11434 \\\n  -v ollama-data:/root/.ollama \\\n  ollama/ollama\n"})}),"\n",(0,s.jsx)(n.h3,{id:"resource-limits",children:"Resource Limits"}),"\n",(0,s.jsx)(n.p,{children:"Prevent resource exhaustion by setting limits:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'docker run -d \\\n  --name ollama \\\n  --network ollama-net \\\n  --memory="8g" \\\n  --cpus="4.0" \\\n  -p 127.0.0.1:11444:11434 \\\n  -v ollama-data:/root/.ollama \\\n  ollama/ollama\n'})}),"\n",(0,s.jsx)(n.h2,{id:"-management-commands",children:"\ud83d\udd27 Management Commands"}),"\n",(0,s.jsx)(n.h3,{id:"container-management",children:"Container Management"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Check container status\ndocker ps\n\n# View logs\ndocker logs ollama\ndocker logs open-webui\n\n# Restart containers\ndocker restart ollama open-webui\n\n# Stop everything\ndocker stop ollama open-webui\n"})}),"\n",(0,s.jsx)(n.h3,{id:"model-management",children:"Model Management"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# List available models online\ndocker exec -it ollama ollama list\n\n# Remove a model\ndocker exec -it ollama ollama rm llama3\n\n# Update a model\ndocker exec -it ollama ollama pull llama3\n"})}),"\n",(0,s.jsx)(n.h3,{id:"data-backup",children:"Data Backup"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Backup model data\ndocker run --rm -v ollama-data:/data -v $(pwd):/backup alpine tar czf /backup/ollama-backup.tar.gz -C /data .\n\n# Backup chat history\ndocker run --rm -v open-webui:/data -v $(pwd):/backup alpine tar czf /backup/webui-backup.tar.gz -C /data .\n"})}),"\n",(0,s.jsx)(n.h2,{id:"-troubleshooting",children:"\ud83d\udea8 Troubleshooting"}),"\n",(0,s.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Ollama won't start:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Check if port is in use\nlsof -i :11444\n\n# Check Docker logs\ndocker logs ollama\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"WebUI can't connect to Ollama:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Verify network connectivity\ndocker exec open-webui ping ollama\n\n# Check environment variables\ndocker exec open-webui env | grep OLLAMA\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Model download fails:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Check available space\ndocker system df\n\n# Try pulling manually\ndocker exec -it ollama ollama pull llama3 --verbose\n"})}),"\n",(0,s.jsx)(n.h3,{id:"performance-issues",children:"Performance Issues"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Slow response times:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Ensure adequate RAM (8GB+ for 7B models)"}),"\n",(0,s.jsx)(n.li,{children:"Consider smaller models (llama3:7b vs llama3:70b)"}),"\n",(0,s.jsx)(n.li,{children:"Enable GPU acceleration if available"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"High memory usage:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Set Docker memory limits"}),"\n",(0,s.jsx)(n.li,{children:"Use smaller models"}),"\n",(0,s.jsx)(n.li,{children:"Clear unused models regularly"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.admonition,{title:"Pro Tip",type:"tip",children:(0,s.jsxs)(n.p,{children:["Models are cached in the ",(0,s.jsx)(n.code,{children:"ollama-data"})," volume. Once downloaded, they persist across container restarts and don't need to be downloaded again!"]})}),"\n",(0,s.jsxs)(n.p,{children:["Next: ",(0,s.jsx)(n.a,{href:"./monitoring",children:"Set up Monitoring \u2192"})]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,l)=>{l.d(n,{R:()=>r,x:()=>i});var a=l(6540);const s={},o=a.createContext(s);function r(e){const n=a.useContext(o);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),a.createElement(o.Provider,{value:n},e.children)}}}]);